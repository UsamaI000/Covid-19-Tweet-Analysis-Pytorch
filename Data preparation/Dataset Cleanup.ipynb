{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dataset Cleanup.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"KVdrjtat6nj_","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from  nltk.stem import SnowballStemmer\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DF17TJGEKz45","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1592547420633,"user_tz":-300,"elapsed":1194,"user":{"displayName":"Muhammad Usama Irfan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_hbwAdkZcAVmx1KZ3mgbzdSsu6wVc5C3FbuGy=s64","userId":"14672832484193415387"}},"outputId":"513cbc2d-a16e-44a3-f442-93433f71f207"},"source":["nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","stemmer = SnowballStemmer('english')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"liLTWrTZ9dcI","colab_type":"code","colab":{}},"source":["def preprocess(text, stem=False):\n","    # Remove link,user and special characters\n","    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n","    tokens = []\n","    for token in text.split():\n","        if token not in stop_words:\n","            if stem:\n","                tokens.append(stemmer.stem(token))\n","            else:\n","                tokens.append(token)\n","    return \" \".join(tokens)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiqZQARp7AdP","colab_type":"code","colab":{}},"source":["def removeOtherLang(data):\n","    english_data = data[data.lang.values == 'en']\n","    return english_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HCCREeu7Hc5","colab_type":"code","colab":{}},"source":["def KeyWordFilter(data):\n","    covid_keys = ['corona', 'covid','covid-19','pandemic','lockdown','stayhome','staysafe','virus', 'wuhan virus', 'curfew']\n","    text = []\n","    for i in data.text:\n","      for key in covid_keys:\n","        if key in i:\n","          text.append(i)\n","    filtered = pd.DataFrame(text, columns=['text'])\n","    return filtered"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nU0Xsxs4FuN7","colab_type":"code","colab":{}},"source":["if __name__ == '__main__':\n","    DATASET_ENCODING = \"ISO-8859-1\"\n","    # TEXT CLEANING\n","    TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n","    \n","    for i in range(1, 8):\n","      data = pd.read_csv(\"corona_tweets_0\"+str(i)+\".csv\", encoding = DATASET_ENCODING)\n","      en_data = removeOtherLang(data)\n","      en_data.text = en_data.text.apply(lambda x: preprocess(x))\n","      filtered_data = KeyWordFilter(en_data)\n","      filtered_data.drop_duplicates(keep='first', inplace=True)\n","      filtered_data.to_csv(\"./drive/My Drive/Filtered Data/Filtered_tweets_0\"+str(i)+\".csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVz3kU8TLuu_","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}